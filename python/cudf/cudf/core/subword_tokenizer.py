from __future__ import annotations
import cupy
from cudf._lib.nvtext.subword_tokenize import (
    subword_tokenize_inmem_hash as cpp_subword_tokenize,
    Hashed_Vocabulary as cpp_hashed_vocabulary,
)


def cast_to_appripate_type(ar, cast_type):
    if cast_type == "cp":
        return ar

    if cast_type == "pt":
        from torch.utils.dlpack import from_dlpack

    elif cast_type == "tf":
        from tf.experimental.dlpack import from_dlpack

    return from_dlpack(ar.astype("int32").toDlpack())


class SubwordTokenizer:
    def __init__(
        self,
        hash_file: str,
        do_lower_case: bool = True,
    ):
        self.do_lower_case = do_lower_case
        self.vocab_file = cpp_hashed_vocabulary(hash_file)

    def __call__(
        self,
        text,
        max_length,
        padding="max_length",
        truncation=False,
        stride=0,
        return_tensors="cp",
        max_num_rows=None,
        return_token_type_ids=False,
        add_special_tokens=False,
    ):
        """
        Run CUDA BERT subword tokenizer on cuDF strings column.
        Encodes words to token ids using vocabulary from a
        pretrained tokenizer.


        Parameters
        ----------
        text: (cudf String Series)
            The sequence or batch of sequences to be encoded.
            Each sequence can be a cudf string series

        add_special_tokens: (bool, optional, defaults to True)
            Whether or not to encode the sequences with the special tokens
            relative to their model.

        max_length: (int)
            Controls the maximum length to use by one of
            the truncation/padding parameters.

        padding: ('max_length')
            Pad to a maximum length specified with the argument max_length

        truncation: (bool, defaults to False)
            True: Truncate to a maximum length specified with the
              argument max_length.
            False or 'do_not_truncate' (default)
              No truncation (Output differs from HuggingFace)

        stride (int, optional, defaults to 0)
            The value of this argument defines the number of
            overlapping tokens.
            The information about the overlapping tokens is
            present in the metadata outputed.

        return_tensors (str) default (cp.ndarray objects)
            'tf': Return TensorFlow tf.constant objects
            'pt': Return PyTorch torch.Tensor objects
            'cp': Return cupy cp.ndarray objects

        max_num_rows(int, optional): Default to twice the number of token-ids
            Maximum number of rows for the output token-ids expected to
            be generated by the tokenizer.
            Used for allocating temporary working memory on the GPU device.
            If the output generates a larger number of rows,
            behavior is undefined.
            This will vary based on stride, truncation, and max_length.
            For example, for non-overlapping sequences output rows will be
            the same as input rows.

        return_token_type_ids (bool, optional): Only False currently supported
        """
        if add_special_tokens:
            # raise not currently supported
            pass

        if return_token_type_ids:
            # raise not currently supported
            # Can also return zeros
            pass

        if truncation in [False, "do_not_truncate"]:
            truncation = False
            # throw warning that the
            # behaviour differs from hugggingFace

        if padding != "max_length":
            # raise appropiate warning
            pass

        if stride > max_length:
            # raise ValueError
            pass

        if return_tensors not in ["cp", "np", "tf"]:
            # raise Notsupported array warning
            pass

        stride = max_length - stride
        # behaviour varies from subword_tokenize but  maps with huggingface

        if max_num_rows is None:
            # picked a arbitary max_value for now
            max_num_rows = len(text) * 2

        input_ids, attention_mask, metadata = cpp_subword_tokenize(
            text._column,
            self.vocab_file,
            max_sequence_length=max_length,
            stride=stride,
            do_lower=self.do_lower_case,
            do_truncate=truncation,
            max_rows_tensor=max_num_rows,
        )

        tokenizer_output = dict()
        tokenizer_output["input_ids"] = cupy.asarray(input_ids).reshape(
            -1, max_length
        )
        tokenizer_output["attention_mask"] = cupy.asarray(
            attention_mask
        ).reshape(-1, max_length)
        tokenizer_output["metadata"] = cupy.asarray(metadata).reshape(-1, 3)

        tokenizer_output = {
            k: cast_to_appripate_type(v, return_tensors)
            for k, v in tokenizer_output.items()
        }

        return tokenizer_output
